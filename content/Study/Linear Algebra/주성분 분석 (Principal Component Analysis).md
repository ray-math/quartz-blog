건강 문제를 다루는 다음 연구를 고려해 보겠습니다. 우리는 연령, 두 가지 콜레스테롤 수치(고밀도 지단백과 저밀도 지단백), 두 가지 혈압 수치(이완기와 수축기), 체중, 키, 운동 습관, 하루 지방 섭취량, 하루 소금 섭취량과 같은 변수들을 사용하여 대규모 사람(피험자) 데이터를 수집합니다. 이 10개의 변수를 두세 개의 새로운 변수로 대체할 수 있다면 훨씬 더 편리할 것입니다. 그러면서도 중요한 정보를 크게 잃지 않도록 하려면 어떻게 해야 할까요?

만약 두 개의 변수가 서로 매우 밀접한 관계를 가지고 있다면, 예를 들어 키와 체중처럼, 이 두 변수를 하나의 새로운 변수로 대체할 수도 있습니다. 이러한 응용에서는 선형 대수를 사용하여 보다 적은 수의 새로운 변수를 찾아냅니다. 우리가 사용하는 방법은 주성분 분석(Principal Component Analysis, PCA) 이라고 하며, 이는 Pearson(1901) 과 Hotelling(1933) 에 의해 개발되었습니다. (자세한 내용은 [4] 및 [1]을 참조하세요.)

### PCA의 주요 실용적 결과
PCA를 사용하면 다음과 같은 실질적인 결과를 얻을 수 있습니다.

1. 많은 양의 데이터를 전송해야 할 경우, PCA를 사용한 변수 축소는 훨씬 적은 양의 데이터로도 본래의 정보를 거의 잃지 않고 전달할 수 있도록 해줍니다.

2. 대부분의 경우, PCA는 변수들의 군집을 보여줍니다. 예를 들어, 양적 점수와 언어적 점수, 또는 대근육 운동 기술과 소근육 운동 기술을 구분할 수 있습니다. 이는 해당 관심 분야에서 유용한 정보를 제공할 수 있습니다.

3. 데이터가 그래프로 표현될 때, PCA는 데이터에 대해 최소 제곱법(least squares sense) 에 가장 적합한 선, 평면 및 초평면을 찾습니다. 또한 데이터를 그래프로 나타낼 때 적절한 축 회전(appropriate rotation of axes) 을 수행하여 최적의 시각화를 제공합니다. 예를 들어, 데이터가 타원형을 따른다면, 타원의 주축과 부축을 좌표축으로 설정하는 것이 바람직할 수 있습니다.

4. 최소 제곱 분석 또는 다른 통계 분석을 수행하는 경우, 적은 수의 변수로 분석을 수행할 수 있습니다. 이는 통계적 검정력(statistical power) 을 증가시키는 효과를 가질 수 있습니다.

PCA에 대해 더 알고 싶다면 [5] 및 [3]을 참조하세요.

### 기초 통계 개념
PCA를 설명하기 전에 몇 가지 기본적인 통계 개념을 짚고 넘어가겠습니다.  
$m$개의 관측값 $x_1, x_2, \dots, x_m$ 가 주어졌을 때, 이 관측값들의 "중심"을 나타내는 익숙한 척도는 (표본) 평균(sample mean) 이며, 이는 다음과 같이 정의됩니다.

$$
\bar{x} = \frac{1}{m} (x_1 + x_2 + \dots + x_m)
$$

예를 들어, 데이터가 $3, 8, 7$ 이라면, 평균은 다음과 같이 계산됩니다.

$$
\bar{x} = \frac{1}{3} (3 + 8 + 7) = 6
$$

그러나 평균만으로는 데이터의 분산(spread) 또는 변동성(variability) 을 측정할 수 없습니다.  
따라서 하나의 합리적인 접근법은 측정값들이 평균에서 얼마나 벗어나 있는지를 제곱하여 평균을 내는 것입니다.  
이를 통해 (표본) 분산(sample variance) 의 정의를 얻을 수 있습니다.

$$
s^2 = \frac{1}{m-1} \sum_{i=1}^{m} (x_i - \bar{x})^2
$$

대부분의 통계 교재에서는 분모에 $m-1$ 을 사용하는 이유 를 설명합니다. 이는 표본이 모집단에서 추출되었을 때, 모집단의 분산을 보다 정확하게 추정할 수 있도록 해주기 때문입니다. (자세한 내용은 [2]를 참조하세요.)

같은 데이터(3, 8, 7)를 사용하여 분산을 계산하면,

$$
s^2 = \frac{1}{2} \left[ (3-6)^2 + (8-6)^2 + (7-6)^2 \right] = 7
$$

가 됩니다.

만약 원래의 측정값이 인치 단위로 된 키(height)라면, 평균 $\bar{x}$ 역시 인치 단위로 주어집니다. 하지만 분산 $s^2$ 은 제곱 인치(square inches) 단위로 표현됩니다.  
원래 데이터와 동일한 단위를 유지하면서 데이터의 변동성을 측정하기 위해, 우리는 분산의 양의 제곱근(positive square root)을 사용합니다. 이 값을 표준 편차(standard deviation) $s$ 라고 하며, 다음과 같이 계산됩니다.

$$
s = \sqrt{7}
$$

행렬(matrix) 표기법을 사용하면 분산을 표현하는 것이 매우 유용합니다.  
관측값 $x_1, x_2, \dots, x_m$ 을 다음과 같은 벡터로 나타낸다고 가정합시다.

$$
X = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_m \end{pmatrix}
$$

그리고 평균 벡터를 다음과 같이 정의합니다.

$$
\bar{X} = \begin{pmatrix} \bar{x} \\ \bar{x} \\ \vdots \\ \bar{x} \end{pmatrix}
$$

이때, 분산 $s_x^2$ 는 다음과 같이 쉽게 표현할 수 있습니다.

$$
s_x^2 = \frac{1}{m-1} (X - \bar{X})^T (X - \bar{X}) = \frac{1}{m-1} \|X - \bar{X}\|^2
$$

(때때로, 관측값이 벡터 $X$ 의 성분으로 표현될 수 있음을 강조하기 위해, 표본 분산(sample variance)을 $s_x^2$ 로 표기하기도 합니다.)

앞서, 두 개의 변수가 매우 밀접한 관계를 가질 경우, 하나의 변수로 대체해도 큰 정보 손실 없이 표현할 수 있다고 언급했습니다.  
이러한 두 변수 간 선형 관계(linear relationship) 의 강도를 측정하는 일반적인 방법 중 하나가 표본 공분산(sample covariance) 입니다.

이를 보다 구체적으로 살펴보겠습니다. 다음과 같이 두 변수 $X$ 와 $Y$ 를 정의합시다.

$$
X = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_m \end{pmatrix}, \quad Y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{pmatrix}
$$

이때, $X$ 와 $Y$ 의 표본 공분산(sample covariance) 은 다음과 같이 정의됩니다.

$$
\text{cov}(X, Y) = \frac{1}{m-1} \sum_{i=1}^{m} (x_i - \bar{x}) (y_i - \bar{y})
$$

행렬 표기법을 사용하면 다음과 같이 간단하게 표현됩니다.

$$
\text{cov}(X, Y) = \frac{1}{m-1} (X - \bar{X})^T (Y - \bar{Y})
$$

또는,

$$
\text{cov}(X, Y) = \frac{1}{m-1} (X - \bar{X}) \cdot (Y - \bar{Y})
$$

하지만 공분산을 사용할 때 발생하는 한 가지 문제점은 측정 단위(units of measurement) 에 따라 값의 크기가 영향을 받는다는 것입니다.  
예를 들어, 만약 $x$ 가 피트(feet) 단위이고, $y$ 가 파운드(pounds) 단위라면, 공분산은 피트-파운드(foot-pounds) 단위가 됩니다.  
반면, 단위를 인치(inches)와 온스(ounces) 로 바꾸면, 공분산의 값이 크게 증가할 수 있습니다.

이러한 문제를 방지하기 위해, 우리는 대신 다음과 같은 상관 계수(correlation coefficient) 를 사용합니다.

$$
\frac{\text{cov}(X, Y)}{s_X s_Y}
$$

여기서 $s_X$ 와 $s_Y$ 는 각각 $X$ 와 $Y$ 의 표준 편차(standard deviation)입니다.  
이러한 비율을 (표본) 상관 계수(sample correlation coefficient) 라고 합니다.  
이는 "단위가 없는(unit-free)" 척도 이며, 조금 더 계산하면 상관 계수의 값이 항상 -1과 1 사이에 위치함을 보일 수 있습니다.

- 완전한 선형 관계(perfectly linear relationship) 인 경우, 즉, 모든 점 $(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)$ 이 직선 위 에 놓여 있다면, 상관 계수는 다음과 같이 결정됩니다.
  - 기울기가 양수 이면 $+1$,
  - 기울기가 음수 이면 $-1$.

- 거의 상관 관계가 없거나(linear 관계가 약한 경우) 상관 계수는 0에 가깝습니다.

### 예제 1

다음과 같은 두 벡터 $X$ 와 $Y$ 가 주어졌다고 가정하겠습니다.

$$
X = \begin{pmatrix} 4 \\ -2 \\ 7 \end{pmatrix}, \quad Y = \begin{pmatrix} 3 \\ 4 \\ 5 \end{pmatrix}
$$

1. $X$ 와 $Y$ 의 평균

$$
\bar{X} = \frac{1}{3} (4 - 2 + 7) = 3, \quad \bar{Y} = \frac{1}{3} (3 + 4 + 5) = 4
$$

2. $X$ 와 $Y$ 의 분산

$$
s_X^2 = \frac{1}{3-1} \left[ (4 - 3)^2 + (-2 - 3)^2 + (7 - 3)^2 \right] = 21
$$

$$
s_Y^2 = \frac{1}{3-1} \left[ (3 - 4)^2 + (4 - 4)^2 + (5 - 4)^2 \right] = 1
$$

3. $X$ 와 $Y$ 의 공분산

$$
\text{cov}(X, Y) = \frac{1}{3-1} \left[ (4 - 3)(3 - 4) + (-2 - 3)(4 - 4) + (7 - 3)(2 - 4) \right] = -\frac{9}{2}
$$

4. $X$ 와 $Y$ 의 상관 계수

$$
\frac{\text{cov}(X, Y)}{s_X s_Y} = \frac{\left(-\frac{9}{2}\right)}{\sqrt{21} \cdot \sqrt{1}} = \frac{-9}{2\sqrt{21}}
$$

## 공분산 행렬(Covariance Matrix)

내적(dot product)의 성질을 이용하면, 공분산 은 다음 성질을 만족합니다.

$$
\text{cov}(X, Y) = \text{cov}(Y, X)
$$

즉, $X$ 와 $Y$ 의 공분산은 $Y$ 와 $X$ 의 공분산과 같습니다.

일반적으로, $n$개의 변수 $X_1, X_2, \dots, X_n$ 이 주어졌을 때, 이를 $m \times 1$ 벡터 로 간주하면,  
$n \times n$ 크기의 행렬 을 정의할 수 있습니다. 이를 공분산 행렬(covariance matrix) 이라고 하며,  
공분산 행렬의 $(i, j)$ 번째 성분은 공분산 $\text{cov}(X_i, X_j)$ 입니다.

행렬 표기법을 사용하여,

$$
X = (X_1 \quad X_2 \quad \dots \quad X_n)
$$

$$
\bar{X} = (\bar{X_1} \quad \bar{X_2} \quad \dots \quad \bar{X_n})
$$

라고 하면, 공분산 행렬은 다음과 같이 표현할 수 있습니다.

$$
C = \frac{1}{m-1} (X - \bar{X})^T (X - \bar{X})
$$

이 공식이 벡터 $X$ 의 분산 $s_X^2$ 과 유사함을 알 수 있습니다.

마찬가지로, 상관 행렬(correlation matrix) 은 $n \times n$ 크기의 행렬 로 정의되며,  
행렬의 $(i,j)$ 번째 성분은 변수 $X_i$ 와 $Y_j$ 사이의 상관 계수 입니다.  
상관 행렬의 형태를 이해하기 위해, 몇 가지 용어를 정의하겠습니다.

- 변수 $Z$ 가 "scaled variable" 이라는 의미:
  - 평균이 0 이고, 표준 편차가 1 인 변수

- 비영(非零) 분산을 가진 변수 $X$ 는 "스케일링(scaled) 또는 표준화(standardized)" 할 수 있습니다.  
  이는 평균 벡터 $\bar{X}$ 를 빼고, 표준 편차 $s_X$ 로 나누는 방식으로 이루어집니다.

$$
Z = \frac{X - \bar{X}}{s_X}
$$

위와 같은 변환을 거친 변수 $Z$ 를 스케일링된 변수(scaled variable) 라고 합니다.

따라서, $n$개의 변수 $X_1, X_2, \dots, X_n$ 이 주어졌을 때,  
이 변수들을 스케일링하여 새로운 변수 $Z_1, Z_2, \dots, Z_n$ 을 얻을 수 있습니다.

이와 같은 스케일링은 일반적으로 변수들이 서로 다른 단위를 가지고 있는 경우 사용됩니다.  
이 과정을 통해 모든 변수를 동일한 기준(equal footing) 위에 놓을 수 있습니다.

만약 다음과 같이 스케일링된 변수 행렬을 정의한다면,

$$
Z = (Z_1 \quad Z_2 \quad \dots \quad Z_n)
$$

원래 변수의 상관 행렬(correlation matrix) 은 다음과 같이 표현됩니다.

$$
C_0 = \frac{1}{m-1} Z^T Z
$$

또한, 다음 성질이 성립합니다.

- 변수 $X_i$ 와 $X_j$ 사이의 상관 계수는 변수 $Z_i$ 와 $Z_j$ 의 상관 계수와 동일합니다.
- 상관 행렬과 공분산 행렬은 대칭 행렬(symmetric matrix)입니다.

### 예제 2

다음은 주어진 텍스트의 한국어 번역입니다.

### 예제 2
다음 표의 데이터는 14명의 Honors Calculus(고급 미적분) 수업 학생들 에 대한 정보입니다.  
4개의 변수는 다음과 같습니다.

- ACT 점수(ACT): 국가 시험 점수(범위: 0 ~ 36)
- FE 점수(FE): 기말 시험 점수(범위: 0 ~ 200)
- Q$_{\text{av}}$: 8개의 퀴즈 점수 평균(범위: 0 ~ 100)
- Tav: 3개의 시험 점수 평균(범위: 0 ~ 100)

이러한 변수들은 마지막 네 개의 열에서 스케일링된 변수(scaled variables) 로 변환되었습니다.  
스케일링된 변수에는 별표(*) 가 추가되어 있습니다.

| Student | ACT | FE  | Qav | Tav | ACT* | FE*  | Qav* | Tav*  |
|---------|----|----|----|----|-----|-----|-----|-----|
| 1 | 33 | 181 | 95  | 89  | 1.27 | 0.94 | 1.30 | 0.95 |
| 2 | 31 | 169 | 81  | 89  | 0.48 | 0.48 | 0.29 | 0.95 |
| 3 | 21 | 176 | 65  | 68  | -1.58 | 0.75 | -0.88 | -0.64 |
| 4 | 25 | 181 | 66  | 90  | -0.63 | 0.94 | -0.81 | 1.03 |
| 5 | 29 | 169 | 89  | 81  | 0.32 | 0.48 | 0.87 | 0.35 |
| 6 | 24 | 103 | 61  | 57  | -0.86 | -2.05 | -1.17 | -1.47 |
| 7 | 24 | 150 | 81  | 76  | -0.86 | -0.25 | 0.29 | -0.03 |
| 8 | 29 | 147 | 86  | 76  | 0.32 | -0.36 | 0.65 | -0.03 |
| 9 | 36 | 181 | 98  | 102 | 1.98 | 0.94 | 1.52 | 1.94 |
| 10 | 26 | 163 | 72  | 70  | -0.39 | 0.25 | -0.37 | -0.49 |
| 11 | 31 | 163 | 95  | 81  | 0.80 | 0.25 | 1.30 | 0.35 |
| 12 | 29 | 147 | 65  | 67  | 0.32 | -0.36 | -0.88 | -0.71 |
| 13 | 23 | 160 | 62  | 68  | -1.10 | 0.14 | -1.10 | -0.64 |
| 14 | 26 | 100 | 63  | 56  | -0.39 | -2.16 | -1.02 | -1.55 |

### 상관 행렬 (Correlation Matrix)

컴퓨터 대수 시스템 MATLAB 을 사용하여

$$
C_0 = \frac{1}{m-1} Z^T Z
$$

을 계산한 결과, $4 \times 4$ 상관 행렬 $C_0$ 을 얻었습니다.  
다음 표는 $C_0$ 의 항목을 소수점 네 자리까지 반올림한 값입니다.

|     | ACT*  | FE*   | Qav*  | Tav*  |
|-----|------|------|------|------|
| ACT*  | 1     | 0.3360 | 0.8111 | 0.7010 |
| FE*   | 0.3360 | 1     | 0.4999 | 0.7958 |
| Qav*  | 0.8111 | 0.4999 | 1     | 0.7487 |
| Tav*  | 0.7010 | 0.7958 | 0.7487 | 1     |

이 행렬을 보면, 다음과 같은 특성을 확인할 수 있습니다.

- ACT 점수와 퀴즈 평균(Qav)의 상관 계수: 0.8111 (높은 양의 상관관계)
- ACT 점수와 시험 평균(Tav)의 상관 계수: 0.7010 (상당한 양의 상관관계)
- 기말 시험(FE) 점수와 시험 평균(Tav)의 상관 계수: 0.7958 (높은 양의 상관관계)
- 퀴즈 평균(Qav)과 시험 평균(Tav)의 상관 계수: 0.7487 (높은 양의 상관관계)

이처럼 상관 행렬을 분석하면 변수들 간의 관계를 명확하게 파악할 수 있습니다.

### 관찰 결과 (Observations)

1. 모든 대각 원소(diagonal entries)는 1 입니다.  
   - 이는 어떤 변수와 자기 자신 간의 상관 계수 가 항상 1 이기 때문입니다.

2. ACT 점수와 퀴즈 평균(Qav) 간의 상관 계수는 0.8111 로 매우 높습니다.

3. ACT 점수와 기말 시험(FE) 간의 상관 계수가 0.3360 으로 가장 약한 상관 관계를 보입니다.

물론, 이 데이터셋은 표본 크기가 매우 작습니다(14명).  
따라서 만약 더 큰 표본을 사용하거나, 동일한 결과가 다른 표본에서도 반복된다면  
일반적인 결론을 도출할 수도 있을 것입니다.

예를 들어, 시험 평균(Tav) 변수는 다른 모든 변수들과 높은 상관 관계 를 가집니다.  
특히, ACT 점수(ACT)와 높은 관련이 있는 변수 중 하나입니다.  
ACT는 대학 입학 전 학생들에게 제공되는 시험이므로, 만약 단 하나의 변수만 사용하여 데이터를 대표해야 한다면,  
Tav를 선택하는 것이 가장 적절할 수도 있습니다.  

## 주성분 분석(Principal Component Analysis, PCA)

이제 예제 2의 데이터를 사용하여 주성분 분석(PCA) 에 대해 논의할 준비가 되었습니다.  
우리는 다음 두 가지 질문에서 시작합니다.

1. 기존 변수를 대체할 새로운 변수를 어떻게 찾을 수 있을까?
2. 이 새로운 변수들이 원래 변수를 얼마나 잘 표현하는지 어떻게 측정할 수 있을까?

이 방법을 설명하기 위해, 원본 데이터셋을 사용 하되,  
이는 다른 데이터셋에도 일반화될 수 있음을 명심하십시오.

- 네 개의 변수 $X_1, X_2, X_3, X_4$ 가 있습니다.  
- 각각은 14 × 1 열 벡터(column vector) 입니다.  
- 변수들의 단위가 다르기 때문에, 스케일링된 변수 $Z_1, Z_2, Z_3, Z_4$ 를 사용하는 것이 더 적절합니다.  
- 즉, 다음과 같이 정의합니다.

$$
Z = (Z_1 \quad Z_2 \quad Z_3 \quad Z_4)
$$

### 4 × 4 상관 행렬 $C_0$

$$
C_0 = \begin{pmatrix}
1.0000 & 0.3360 & 0.8111 & 0.7010 \\
0.3360 & 1.0000 & 0.4999 & 0.7958 \\
0.8111 & 0.4999 & 1.0000 & 0.7487 \\
0.7010 & 0.7958 & 0.7487 & 1.0000
\end{pmatrix}
$$

우리가 앞서 논의한 대로, 이 행렬은 반드시 대칭(symmetric)입니다.  
(즉, $C_0$ 의 $(i,j)$ 항목은 $(j,i)$ 항목과 동일합니다.)

### 고유값 분해(Eigenvalue Decomposition)

이제 정리 6.20(Theorem 6.20) 에 따라,  
상관 행렬 $C_0$ 에는 직교 정규 기저(orthonormal basis) 가 존재합니다.

즉, $C_0$ 의 고유벡터(eigenvectors) $U_1, U_2, U_3, U_4$  를 찾을 수 있습니다.  
이에 대응하는 고유값(eigenvalues) $\lambda_1, \lambda_2, \lambda_3, \lambda_4$  을 찾을 수 있습니다.

우리는 MATLAB 을 사용하여 아래의 직교 행렬(orthogonal matrix) $P$ 와  
대각 행렬(diagonal matrix) $D$ 을 얻었습니다.

### 고유벡터 행렬 $P$
(각 열은 $C_0$ 의 고유벡터)

$$
P = \begin{pmatrix}
0.4856 & -0.5561 & 0.5128 & 0.4381 \\
0.4378 & 0.7317 & -0.0640 & 0.5185 \\
0.5209 & -0.3275 & -0.7848 & -0.0744 \\
0.5489 & 0.2192 & 0.3421 & -0.7305
\end{pmatrix}
$$

### 고유값 행렬 $D$

$$
D = \begin{pmatrix}
2.9654 & 0 & 0 & 0 \\
0 & 0.7593 & 0 & 0 \\
0 & 0 & 0.1844 & 0 \\
0 & 0 & 0 & 0.0910
\end{pmatrix}
$$

첫 번째 새로운 변수는 첫 번째 주성분(principal component) 이라고 하며, 다음과 같이 정의됩니다.

$$
Y_1 = Z U_1
$$

여기서 $U_1$ 은 가장 큰 고유값(2.9654)을 가진 $C_0$ 의 고유벡터(eigenvector) 입니다.

두 번째 주성분은

$$
Y_2 = Z U_2
$$

로 정의되며, $U_2$ 은 두 번째로 큰 고유값(0.7593)을 가진 고유벡터 입니다.

### 첫 번째와 두 번째 주성분

$$
Y_1 = 0.4856Z_1 + 0.4378Z_2 - 0.5209Z_3 + 0.5489Z_4
$$

$$
Y_2 = -0.5561Z_1 + 0.7317Z_2 - 0.3275Z_3 + 0.2192Z_4
$$

이를 원래 스케일링된 변수 $ACT^*, FE^*, Qav^*, Tav^*$ 로 나타내면,

$$
Y_1 = 0.4856 ACT^* + 0.4378 FE^* - 0.5209 Qav^* + 0.5489 Tav^*
$$

$$
Y_2 = -0.5561 ACT^* + 0.7317 FE^* - 0.3275 Qav^* + 0.2192 Tav^*
$$

### 주성분 해석

- 주어진 계수(coefficient)는 고유벡터의 성분을 의미하며, "로딩(loading)" 이라고 불립니다.
- 각 변수의 계수 값은 해당 변수가 특정 주성분과 얼마나 관련이 있는지를 나타냅니다.
- 첫 번째 주성분 $Y_1$ 은 네 개 변수의 가중 평균(weighted average) 에 가까운 형태로,  
  각 변수의 가중치가 거의 동일하게 분포 되어 있습니다.
- 반면 두 번째 주성분 $Y_2$ 은 기말 시험(FE)과 시험 평균(Tav)의 쌍과, ACT와 퀴즈 평균(Qav)의 쌍 사이의 대비(contrast) 를 나타냅니다.  
  - 즉, FE와 Tav의 계수는 양수, ACT와 Qav의 계수는 음수 로 나타나며, 서로 반대 방향의 영향을 줍니다.

### 통계적 해석: 분산 설명 능력

주성분을 사용할 때 가장 중요한 기준 중 하나는,  
새로운 변수가 원래 데이터의 분산(variance)을 얼마나 설명하는지 여부입니다.

- 직관적으로, 변수를 줄이더라도 데이터의 전체 변동성이 크게 줄어들지 않는 것이 중요 합니다.
- 각 주성분의 분산은 해당 주성분의 고유값(eigenvalue)과 동일 합니다.

즉,

- 첫 번째 주성분의 분산:

$$
\text{Var}(Y_1) = \lambda_1 = 2.9654
$$

- 전체 분산(고유값의 합):

$$
2.9654 + 0.7593 + 0.1844 + 0.0910 = 4.0000
$$

- 따라서, $Y_1$ 이 설명하는 분산의 비율:

$$
\frac{2.9654}{4.0000} = 74.14\%
$$

- 첫 번째와 두 번째 주성분이 설명하는 총 분산:

$$
\frac{2.9654 + 0.7593}{4.0000} = 93.12\%
$$

즉, 첫 번째와 두 번째 주성분만으로 원래 데이터의 93.12%를 설명할 수 있습니다.  
이는 네 개의 원래 변수를 모두 사용하는 것보다 훨씬 효율적입니다.

### 주성분과 원래 변수 간의 상관 관계

다음 표는 스케일링된 변수와 첫 번째 및 두 번째 주성분 간의 상관 계수 를 나타냅니다.  
(소수점 네 자리까지 반올림)

|     | ACT*  | FE*   | Qav*  | Tav*  | $Y_1$ | $Y_2$ |
|-----|------|------|------|------|------|------|
| ACT*  | 1.0000 | 0.3660 | 0.8111 | 0.7010 | 0.8362 | -0.4846 |
| FE*   | 0.3360 | 1.0000 | 0.4999 | 0.7958 | 0.7539 | 0.6376 |
| Qav*  | 0.8111 | 0.4999 | 1.0000 | 0.7487 | 0.8969 | -0.2854 |
| Tav*  | 0.7010 | 0.7958 | 0.7487 | 1.0000 | 0.9452 | 0.1911 |
| $Y_1$  | 0.8362 | 0.7539 | 0.8969 | 0.9452 | 1.0000 | 0.0000 |
| $Y_2$  | -0.4846 | 0.6376 | -0.2854 | 0.1911 | 0.0000 | 1.0000 |

### 상관 관계 분석
- $Y_1$ 과 Tav* (시험 평균)의 상관 계수는 0.9452  
  → 시험 평균이 첫 번째 주성분에 가장 중요한 영향을 줌 을 의미
- $Y_2$ 와 Tav* 의 상관 계수는 0.1911  
  → 시험 평균이 두 번째 주성분에는 거의 기여하지 않음
- $Y_1$ 과 $Y_2$ 간의 상관 계수는 0  
  → 주성분은 상호 독립적(uncorrelated) 임을 의미

## 결론 및 코멘트

- 주성분 분석(PCA)은 고차원 데이터를 더 적은 차원의 데이터로 축소하는 기법 입니다.  
  - 원래 데이터의 대부분의 정보를 유지하면서 효율적으로 변수 수를 줄일 수 있습니다.
- 각 주성분은 원래 변수들의 선형 결합(linear combination)으로 표현됩니다.
- 첫 번째 몇 개의 주성분만으로도 전체 데이터의 분산을 높은 비율로 설명할 수 있습니다.  
  - 이 경우, 첫 번째와 두 번째 주성분만으로 93.12% 설명 가능
- 주성분은 상호 독립적(uncorrelated)입니다.
- 주성분은 해석이 직관적이지 않을 수도 있습니다.  
  - $Y_1$ 은 원래 변수들의 평균과 유사한 역할을 하지만,  
    $Y_2$ 는 특정 변수들의 대비(contrast)를 나타냅니다.
- 이론적 분석을 위해 변수들의 확률 분포(statistical distribution)에 대한 가정은 하지 않았습니다.  
  - 이 주제는 보다 깊은 수학적 통계적 배경 이 필요합니다.
