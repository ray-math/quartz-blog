---
title: 모든 것을 알기위한 열쇠, 독립
date: 2023-10-25
tags:
  - 독립
  - 오류
  - 전체
  - 개수
  - 확률
  - 방법
  - 이용
  - 비율
  - 발견
  - 검사
---
**독립**이라는 단어는 일상적인 언어에서  

> 다른 것에 예속하거나 의존하지 아니하는 상태로 됨.

이라는 의미로 사용되곤 합니다. 하지만 그 의미 때문에 확률과 통계에서 독립의 개념을 배울 때한 사건이 다른 사건과 아무런 관련이 없음을 의미한다고 착각하기 쉽습니다. 하지만 수학에서 독립은 한 사건의 발생이 다른 사건의 발생 확률에 영향을 미치지 않는 것을 의미합니다. 다시 말해, 어떤 조건에서든지 확률이 일정하게 유지되는 성질을 의미하는 것이지 배반사건
을 의미하는 것이 아닙니다.

두 사건 $A$와 $B$가 독립이라는 개념을 조건부 확률식을 이용해 나타내면

$$
P(A \vert B) = P(A)
$$

라 할 수 있습니다. $A$가 일어날 확률은 $B$가 일어난다고 해도 바뀌지 않는다는 거죠. 따라서 $A$와 $B$가 독립이라면 증명하지 않아도 아래 식들 또한 자명해집니다.

$$
\begin{align*}
P(A \vert B^c) = P(A)\\
P(B \vert A) = P(B)\\
P(B \vert A^c) = P(B)\\ 
\end{align*}
$$

이 개념을 처음 접하는 학생들은 문제에 바로 적용하기에는 어려움이 있어 학교에서는 조건부 확률의 성질을 이용해 필요충분조건인 식을 가르치죠.  

$$
\begin{align*}
&P(A|B) = P(A) \\
&\Leftrightarrow \frac{P(A \cap B)}{P(B)} = P(A) \left( \because P(A|B) = \frac{P(A \cap B)}{P(B)} \right)\\
&\Leftrightarrow P(A \cap B) = P(A) \cdot P(B) 
\end{align*}
$$

그래서 독립이란 교집합을 곱셈으로 구할 수 있다 정도로만 생각하고 끝내기 십상입니다. 하지만 이러한 식은 독립이 갖고 있는 의미를 전혀 반영하지 못합니다. 독립은 하나의 변수가 다른 변수에 미치는 영향을 고려하지 않아도 되므로, 분석을 단순화할 수 있다는 큰 장점이 있습니다. 표본들이 독립이라는 가정 하에, 평균, 분산 등의 통계량을 추정하기 용이하여 모집단에 대한 일반화가 가능하죠.  
 
## 독립개념을 확장하기

우선 독립의 필요충분 조건을 다시 보도록 하겠습니다.  이 식 자체로도 굉장히 깔끔하지만 저는 이 식의 변수를 조금 줄여보도록 하겠습니다. 

$$
P(A \cap B) = P(A) \cdot P(B)
$$

확률의 정의를 사용하면 전체 사건 $S$에 대해 각각의 확률을 집합의 원소의 수를 이용해 정의할 수 있습니다.

$$
\begin{align*}
P(A) &= \frac{n(A)}{n(S)} \\
P(B) &= \frac{n(B)}{n(S)} \\
P(A \cap B) &= \frac{n(A \cap B)}{n(S)}
\end{align*}
$$

이제 두 사건 $A$와 $B$가 독립이므로 독립의 필요충분조건 식에 확률을 대입한 후에 양변에 전체 사건의 경우의 수$n(S)$를 곱하여 정리하면 $n(S)$에 대한 식을 얻을 수 있습니다.

$$
\begin{align*}
&\frac{n(A \cap B)}{n(S)} = \left( \frac{n(A)}{n(S)} \right) \cdot \left( \frac{n(B)}{n(S)} \right) \\
\\
&\Rightarrow n(A) \cdot n(B) = n(A \cap B) \cdot n(S) \\
\\
&\Rightarrow n(S) =\frac{n(A) \cdot n(B)}{n(A \cap B)}
\end{align*}
$$
## 독립성 확인

예를 들어보죠. $1$부터 $8$까지의 카드가 있을 때, 집합$A$와 $B$를 다음과 같이 정의하겠습니다.

- $S=\{1,~2,~3,~4,~5,~6,~7,~8\}$
- $A=\{ 1,~2,~3,~4\}$
- $B=\{3,~4,~5,~6\}$

이 두 집합은 독립일까요? 

### 확률을 이용한 독립성 확인

가장 일반적인 방법으로 확률을 이용해 독립성을 확인해보겠습니다. 전체에 대해 $A$와 $B$의 확률을 계산하면 $P(A)$는 전체 $8$개 중 $4$개이므로 $P(A)=\frac{4}{8}=\frac{1}{2}$이고 $P(B)$또한 전체 $8$개 중 $4$개이므로 $P(B)=\frac{1}{2}$입니다. 이제 $P(A \cap B)$를 보면 전체 $8$개 중에서 $A$와 $B$에 동시에 포함된 원소는 $\{3,~4\}$ 즉, $2$개가 있으므로 $P(A \cap B)=\frac{2}{8}=\frac{1}{4}$입니다. 따라서  $P(A)P(B)=P(A \cap B)$를 만족하므로, 독립이라고 할 수 있습니다.

$$
\begin{align*}
P(A) &= \frac{n(A)}{n(S)} = \frac{4}{8} = \frac{1}{2} \\
P(B) &= \frac{n(B)}{n(S)} = \frac{4}{8} = \frac{1}{2} \\
P(A \cap B) &= \frac{n(A \cap B)}{n(S)} = \frac{2}{8} = \frac{1}{4}
\end{align*}
$$

$$
P(A)\times P(B) = \frac{1}{2} \times \frac{1}{2} = \frac{1}{4} = P(A \cap B)
$$

### 비율을 이용한 독립성 확인

독립을 판단하는 방법은 확률 관계뿐만 아니라 비율을 보는 방법도 있습니다. $A$는 원소의 개수가 $4$개 이므로 전체에서 절반의 비율을 차지하고 있습니다. 만약 $A$와 $B$가 독립이라면 $A$는 $B$안에서나 밖에서나 일정하게 그 비율을 유지해야합니다. $B$의 원소 $\{3,~4,~5,~6\}$에 대해 $\{3,~4\}$는 $A$의 원소이기도 합니다. 따라서 $B$를 전체라고 본다면, $A$는 $B$안에서도 절반의 비율을 차지하고 있습니다. 그러므로 $A$는 $B$에 관계 없이 비율을 항상 일정하게 유지하고 있으므로 독립이라고 할 수 있습니다.

$$
\begin{align*}
P(A) = \frac{n(A)}{n(S)} = \frac{4}{8} &= \frac{1}{2} \\
P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{n(A \cap B)}{n(B)} &= \frac{2}{4}= \frac{1}{2} = P(A) \\
P(A|B^c) = \frac{P(A \cap B^c)}{P(B^c)} =  \frac{n(A \cap B^c)}{n(B)} &= \frac{2}{4}= \frac{1}{2} = P(A)
\end{align*}
$$

이러한 이유로 독립을 '비율 일정의 법칙'이라 부르기도 합니다. 왜 비율이 일정하다고 하는지는 빈도 교차표를 보면 더 눈에 띄게 드러납니다. 1번 표를 보면 남자에서는 미적과 확통의 비율이 $3:7$인 반면 여자에서는 비율이 $5:11$로 같지 않은 것을 알 수 있습니다. 따라서 남자와 여자는 미적과 확통에 대해 독립적이지 않다는 것을 알 수 있습니다. 

$$
\begin{array}{|c|c|c|c|} 
\hline
\text{\#1} & \text{미적} & \text{확통} & \text{합계} \\ \hline
\text{남자} & 3 & 7 & 10 \\ \hline
\text{여자} & 5 & 11 & 16 \\ \hline
\text{합계} & 8 & 18 & 26 \\ \hline
\end{array}
$$

반면에 2번 표를 보면 남자에서는 미적과 확통의 비율이 $1:3$이며 이는 여자일 때도 비율이 일정하게 유지 되는 것을 알 수 있습니다. 따라서 남자와 여자는 미적과 확통에 대해 독립적이라고 할 수 있습니다. 이는 미적과 확통을 기준으로 남자와 여자의 비율을 보아도 $5:4$로 일정하게 유지되는 것을 볼 수 있습니다. 
 
$$
\begin{array}{|c|c|c|c|} \hline
\text{\#2} &\text{미적} & \text{확통} & \text{합계} \\ \hline
\text{남자} & 5 & 15 & 20 \\ \hline
\text{여자} & 4 & 12 & 16 \\ \hline
\text{합계} & 9 & 27 & 36 \\ \hline
\end{array}
$$

이러한 성질을 알고 있다면 $A$와 $B$가 독립이라는 정보가 주어졌을 때, 비율을 이용해 미지의 값을 빠르게 구할 수 있습니다. 일반화하여 적용하는 방법은 잠시 후에 자세히 다루도록 하겠습니다.

$$
\begin{array}{|c|c|c|c|}
\hline
\text{\#3} & \quad A \quad & \quad A^c \quad  & \quad  \text{합계} \quad  \\
\hline
B & 3 & 4 & 7 \\
\hline
B^c & 6 & x & 6+x \\
\hline
\text{합계} & 9 & 4+x & 13+2x \\
\hline
\end{array}
$$

$$
3 : 4 = 6 : x  \Rightarrow  x = 8
$$

### 전체 경우의 수로 확인

마지막으로 이전에 유도한 공식을 이용할 수도 있습니다. $A$와 $B$의 원소의 개수를 곱한 값을 $A \cap B$의 경우의 수로 나눈 값이 전체 경우의 수가 되는지 확인하는 방식을 이용하면 이전에 방법과 유사하지만 적은 연산으로도 빠르게 독립성을 판단할 수 있습니다.

$$
\frac{n(A) \cdot n(B)}{n(A \cap B)} = \frac{4 \times 4}{2} = 8 = n(S)
$$

## 포획-재포획법

앞서 보았듯 독립이란 조건을 이용하면 집합 $A$와 $B$만으로도 전체 사건의 경우의 수 즉, 전체 집합의 원소의 개수를 구할 수 있습니다. 예를 들어보죠. 

$$
n(S)=\frac{n(A) \cdot n(B)}{n(A \cap B)}
$$

코드 작성은 어려운 작업 중 하나입니다. 특히 복잡한 프로젝트에서는 수많은 줄의 코드가 얽혀 있으며, 그 중 하나라도 잘못되면 전체 시스템이 작동하지 않을 수 있죠. 이런 상황에서 모든 오류를 찾는 것은 정말 어려운 일입니다. 만약 엄청 복잡한 코드가 실행되지 않는 상황이 발생했다고 해보겠습니다. 두 프로그래머 $A$와 $B$가 이 문제를 해결하기 위해 코드 **전체**를 읽어보며 오류를 찾기로 했습니다. 이때, $A$와 $B$는 각자 $50$개와 $40$개의 오류를 발견했다고 해보겠습니다. 다행히 두 프로그래머가 분석한 오류 중 중복으로 발견한 오류가 있었고, 그 개수는 $10$개였습니다. 여기서 두 프로그래머의 작업은 독립적(각자 오류를 찾을 확률이 일정하다고 했을 때)이므로, 전체 오류의 개수   를 찾을 수 있습니다.

$$
n(S) = \frac{n(A) \cdot n(B)}{n(A \cap B)} = \frac{50 \times 40}{10} = 200
$$

$A$가 발견한 오류와 $B$가 발견한 오류를 곱하고, 그 결과를 둘 다 발견한 오류로 나누면 되죠. 결과적으로 전체 코드에서의 오류 개수는 $200$개로 추정할 수 있습니다. 여기서 한 발 더 나아가 보겠습니다. 현재 발견한 코드의 오류 개수는 중복 영역에서 발견한 오류를 한 번만 세어야 하므로 합집합 개념을 이용해 표현할 수 있습니다.

$$
n(A \cup B) = 50 + 40 - 10 = 80
$$

$A$와 $B$가 현재 발견한 오류의 개수는 총 $80$개이고 따라서 두 프로그래머가 앞으로 전체 코드에서 발견해야 할 오류의 개수가 $120$개 라는 것을 파악할 수 있습니다.

$$
n((A \cup B)^c)=n(S) - n(A \cup B) = 200 - 80 = 120
$$

이 과정을 일반화 해보죠. 먼저 전체 오류의 개수와 현재 발견한 오류의 개수를 구하는 식은 다음과 같이 정의했습니다.

$$
n(S) = \frac{n(A) \cdot n(B)}{n(A \cap B)}
$$
$$
n(A \cup B) = n(A) + n(B) - n(A \cap B)
$$

앞으로 찾아야 할 오류의 개수는 합집합의 여집합이므로 계산하는 식에 위의 두 식을 대입할 수 있습니다.

$$
\begin{align*}\\
n((A \cup B)^c) &= n(S) - n(A \cup B)\\
 &= \frac{n(A) \cdot n(B)}{n(A \cap B)} - (n(A) + n(B) - n(A \cap B)) \end{align*}
 $$

두 프로그래머가 찾은 오류 중 중복되지 않은 오류의 개수를 각각 $n(A - B)$와 $n(B - A)$로 표현하고, 둘 다 찾은 오류의 개수를 $n(A \cap B)$로 표현하면, 앞으로 찾아야 할 오류의 개수는 다음과 같이 계산됩니다

$$
\begin{align*}
n((A \cup B)^c) &= \frac{n(A) \cdot n(B) - n(A \cap B) \cdot (n(A) + n(B) - n(A \cap B))}{n(A \cap B)} \\
&= \frac{n(A) \cdot n(B) - n(A \cap B) \cdot n(A) - n(A \cap B) \cdot n(B) + (n(A \cap B))^2}{n(A \cap B)} \\
&= \frac{n(A) \cdot n(B) - n(A) \cdot n(A \cap B) - n(B) \cdot n(A \cap B) + n(A \cap B) \cdot n(A \cap B)}{n(A \cap B)} \\
&= \frac{(n(A) - n(A \cap B)) \cdot (n(B) - n(A \cap B))}{n(A \cap B)}\\
&= \frac{(n(A- B)) \cdot (n(B-A))}{n(A \cap B)}
\end{align*}
$$

이렇게 정리된 식은 우리에게 많은 정보를 줍니다. 먼저 이 식을 통해 $A$와 $B$가 각각 자신들만 찾은 오류가 많고, 둘 다 찾아낸 오류의 개수가 적다면, 여전히 많은 오류가 남아 있을 것이라는 정보를 줍니다. 그리고 이 식에는 두 프로그래머가 얼마나 오류를 잘 찾아내는지를 나타내는 정확도에 대한 내용이 없습니다. 따라서 $A$와 $B$가 오류를 찾는 정확도가 낮다고 하더라도 독립적이라는 성질만으로 전체를 꽤 정확하게 추측 할 수 있음을 보여줍니다.

## How many words did Shakespeare know?

우리가 본 것만으로 보지 못한 것을 추론할 수 있을까요? 이 흥미로운 질문에 통계학자 에프론(Bradley Efron)과 티스티드(Ronald Thisted)는 셰익스피어의 어휘력을 분석하며 답을 찾아 나섰습니다. 그들이 사용한 방법은 마치 생태학자가 숲속 동물의 총개체 수를 추정하는 것과 같았죠.

먼저 그들은 셰익스피어가 남긴 방대한 데이터와 마주했습니다. 총 $884,647$개의 단어로 이루어진 작품들 속에서, 중복을 제외한 고유 단어(word type)는 $31,534$개였습니다. 여기서 $n_x$를 정확히 $x$번 등장한 단어의 종류 수라고 할 때, 데이터는 아주 특이한 패턴을 보였습니다. 한 번만 쓰인 단어($n_1$)가 $14,376$개로, 전체의 $45\%$를 넘었습니다.

$$
\frac{n_1}{\sum n_x} = \frac{14,376}{31,534} \approx 45.6\%
$$

이렇게 스치듯 사용된 단어가 많다는 것은, 셰익스피어의 머릿속에는 아직 지면으로 나오지 못한 단어들이 훨씬 더 많았을 것이라는 강력한 암시였죠.

그렇다면 셰익스피어가 지금과 똑같은 분량의 글을 한 번 더 썼다면($t=1$), 새로운 단어는 몇 개나 등장할까요? 이 질문에 답하기 위해, 먼저 각 단어의 등장 확률을 모델링해야 합니다. 셰익스피어가 아는 모든 단어($S$개)는 각각 고유의 사용 빈도($\lambda_s$)를 가지며, 특정 작품에서 해당 단어가 등장하는 횟수는 포아송 분포를 따른다고 가정합니다. 

$$
\Delta(t) = S\int_{0}^{\infty}e^{-\lambda}(1-e^{-\lambda t})dG(\lambda)
$$

여기서 $G(\lambda)$는 셰익스피어가 아는 모든 단어의 사용 빈도($\lambda$) 분포로, 우리가 알지 못하는 핵심 정보입니다. 여기서 굿과 툴민(Good & Toulmin)은 이 식의 이론적 기대값($\eta_x = E(n_x)$)이 $\Delta(t) = \eta_1 t - \eta_2 t^2 + \eta_3 t^3 - \cdots$ 와 같다는 것을 보였고 , 이를 통해 관측값 $n_x$를 사용한 추정식을 얻을 수 있습니다.

$$
\Delta(t) \approx n_1 t - n_2 t^2 + n_3 t^3 - \dots = \sum_{x=1}^{\infty} (-1)^{x+1} n_x t^x

$$

이 공식에 셰익스피어의 데이터를 넣으면, 한 권의 책을 더 썼을 때, 약 $11,430$개의 새로운 단어가 나타날 것이라는 추정치를 얻게 됩니다.

$$
\hat{\Delta}(1) = 14376 - 4343 + 2292 - \cdots \approx 11,430 \quad (\text{표준편차} \approx 178)

$$

하지만 이 추정식은 $t$가 1보다 커지면 값이 미친 듯이 널뛰며 발산하기 때문에, 셰익스피어의 '전체' 어휘량($t \to \infty$)을 알아내는 데는 쓸모가 없습니다. 

$$
\hat{\Delta}^{x_{0}}(u)=\sum_{v=1}^{x_{0}}\hat{\xi}_{v}u^{v}
$$

이 문제를 해결하기 위해 저자들은 오일러 변환(Euler's transformation)이라는 무기를 꺼내 듭니다. 불안정한 급수의 항들을 특정 가중치로 평균 내어 안정적인 값으로 수렴시키는 기법입니다. 이 변환을 통해 얻은 새로운 추정치는 기존 값과 거의 차이가 없으면서도, 표준편차가 줄어들어 훨씬 안정적인 결과가 되었습니다.

$$
\hat{\Delta}^{9}(1) \approx 11,441 \quad (\text{표준편차} \approx 147)
$$

셰익스피어가 알았지만 사용하지 않은 단어의 총 수($\Delta(\infty)$)에 대한 상한선은 정할 수 없습니다. 따라서 저자들은 "아무리 보수적으로 추정해도 최소한 이 정도는 넘을 것이다"라는 하한선(lower bound)을 찾는 데 집중했습니다.

$$
\text{Minimize: } \Delta(\infty) = S\int_{0}^{\infty}e^{-\lambda}dG(\lambda)
$$

아이디어는 이렇습니다. 우리가 만든 추정치 $\hat{\Delta} = \sum h_x n_x$가 실제 값 $\Delta(t)$보다 항상 작거나 같도록 보장하는 제약조건 하에서($E(\hat{\Delta}) \le \Delta(t)$), 추정값을 최대로 만드는 계수($h_x$)를 찾아내는 것이죠. 즉, 관측된 데이터와 모순되지 않는 가장 비관적인 시나리오를 상정해 답을 구하는 것입니다.

$$
\hat{\eta}_{x}-c\sqrt{\hat{\eta}}_{x} \le S\int_{0}^{\infty}(\frac{e^{-\lambda}\lambda^{x}}{x!})dG(\lambda) \le \hat{\eta}_{x}+c\sqrt{\hat{\eta}}_{x} \quad (x=1, \cdots ,x_{0})
$$

결과적으로 셰익스피어가 알고 있었지만 단 한 번도 쓰지 않은 단어가 최소 $35,554$개는 넘는다는 것을 알 수 있습니다.

우리가 보지 못한 것을 마주할 때, "그건 안 보여서 몰라요"라고 말하기 전에, 이렇게 다시 묻는 것도 좋겠습니다. "관측된 걸 보면, 보이지 않는 것도 알 수 있지 않을까요?"

[^1]: [Estimating the number of unseen species: How many words did Shakespeare know?](https://doi.org/10.1093/biomet/63.3.435)